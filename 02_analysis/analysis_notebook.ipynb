{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd698d1-a6a2-4ca6-a8af-b0bbc0f2679e",
   "metadata": {},
   "source": [
    "# Framing Poverty in Everyday Conversations: Insights from Reddit\n",
    "\n",
    "**Author:** Darius Nader \\\n",
    "**Date:** 22.08.2025  \n",
    "\n",
    "---\n",
    "\n",
    "#### Purpose of this Notebook\n",
    "\n",
    "This notebook provides the code for the term paper *“Framing Poverty in Everyday Conversations: Insights from Reddit.”* It provides a step-by-step workflow to replicate the main results. The analysis covers data preparation, cleaning, topic modeling (LDA), framing classification, sentiment analysis and visualization.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Important information\n",
    "\n",
    "- Place the dataset (`reddit_posts.jsonl`) inside a dedicated folder\n",
    "- Update the working directory at the beginning of the notebook\n",
    "- Make sure all required Python packages are installed before running the notebook\n",
    "- The notebook has been designed so that it works when executed from top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96a950-1457-4085-b97e-784ecb3e620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install pandas regex nltk gensim scikit-learn numpy seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d053d54-9c7e-4822-a10f-f72521826d33",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Importing Data and Pre-Processing\n",
    "\n",
    "We begin by loading the Reddit dataset that underpins the paper’s analysis. The raw data comes in JSON Lines (`.jsonl`) format and contains posts from the `r/poor` subreddit between January 1, 2023 and August 3, 2025.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f993a-b83e-444b-8dd6-8b4e82ee00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the working directory to where the dataset was saved\n",
    "%cd \"/Users/dn/poverty-frames-reddit/01_data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45281b1c-82d5-4e36-8770-2d999504406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd  \n",
    "\n",
    "# Load dataset (json lines format, each line = one reddit post)\n",
    "df = pd.read_json(\"reddit_posts.jsonl\", lines=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "columns_to_keep = [\n",
    "    \"id\", # needed for merge\n",
    "    \"title\",\n",
    "    \"selftext\",\n",
    "    \"created_utc\",\n",
    "    \"removed_by_category\",\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Quick inspection of structure and first rows\n",
    "df.info()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e872c2e-4649-4846-935a-0267db814e44",
   "metadata": {},
   "source": [
    "We now have 7,671 posts in our dataset. Many entries are spam-like, deleted or removed by moderators, these will need further preprocessing in the next step.\n",
    "\n",
    "Now we prepare the dataset for analysis by:\n",
    "- converting timestamps to datetime  \n",
    "- excluding posts after July 2025 (not part of the time period of the study)  \n",
    "- removing posts deleted or flagged by moderators/Reddit  \n",
    "- dropping helping variables no longer needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d578-27b0-43ca-b601-4382c6b3bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"created_utc\"], unit=\"s\")\n",
    "\n",
    "# Drop posts after July 2025\n",
    "df = df[~((df[\"datetime\"].dt.year == 2025) & (df[\"datetime\"].dt.month == 8))]\n",
    "\n",
    "# Filter out removed/deleted posts\n",
    "df = df[~df[\"selftext\"].str.contains(\"[removed]\", regex=False, na=False)]\n",
    "df = df[~df[\"removed_by_category\"].str.contains(\"moderator|reddit\", na=False)]\n",
    "\n",
    "# Drop columns that are no longer needed\n",
    "df = df.drop(columns=[\"removed_by_category\", \"created_utc\"])\n",
    "\n",
    "# Quick check\n",
    "df.info()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f28162-4019-4ab7-9f4e-39b53af8b8d5",
   "metadata": {},
   "source": [
    "The total of number of posts in the sample is now 5,255 between January 1, 2023 and July 30, 2025. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cbdb6-921b-42c0-865b-f81ebf6e99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "output_path = \"/Users/dn/poverty-frames-reddit/01_data/processed/processed_posts.jsonl\"\n",
    "df.to_json(output_path, orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e421420-9cb2-4fca-81c0-80cfa60a38a5",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Descriptive Statistics of Sample\n",
    "\n",
    "Before diving into preprocessing, we take a quick look at the descriptive statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f76423-7dcc-4712-b2e8-61fc8d3840a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dataset\n",
    "processed_path = \"/Users/dn/poverty-frames-reddit/01_data/processed/processed_posts.jsonl\"\n",
    "df_processed = pd.read_json(processed_path, lines=True)\n",
    "\n",
    "# Create simple summary table\n",
    "\n",
    "print(\"Begin:\", df_processed[\"datetime\"].min().date())\n",
    "print(\"End:\", df_processed[\"datetime\"].max().date())\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Count threads per year\n",
    "print(df_processed[\"datetime\"].dt.year.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857748c-4af0-4e71-96dc-9863961456d9",
   "metadata": {},
   "source": [
    "The dataset now contains posts, spanning from January 2023 to July 2025. Looking at the annual distribution, there are 1,486 posts in 2023, 2,551 posts in 2024 and 1,218 posts in 2025 (up to July). \n",
    "This confirms that the cleaned sample covers the intended study period and provides enough observations for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09755b-5f16-4214-8d52-0e9807561d3c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Processing Text for the Data Analysis\n",
    "\n",
    "To prepare the texts for topic modeling and the sentiment analysis, we first combine the `title` and `selftext` fields into a single string.  A quick sample of posts shows that the raw data often contains promotional spam, referral links, HTML artifacts, or deleted content, which must be cleaned before starting the data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9288de-32c2-4640-869a-d5d77ce4505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and selftext into a single field\n",
    "df_processed[\"combined_text\"] = df_processed[\"title\"] + \" \" + df_processed[\"selftext\"]\n",
    "\n",
    "# Preview three random examples to illustrate raw content\n",
    "for r, row in df_processed.sample(3, random_state=123)[[\"combined_text\"]].iterrows():\n",
    "    print(row[\"combined_text\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d822d3-b3d6-45b6-bc04-e02bf3ba3c27",
   "metadata": {},
   "source": [
    "Now we need to clean the combined text more thoroughly. The raw posts still contain a number of artifacts that would interfere with the data analysis, such as:  \n",
    "\n",
    "- Referral codes and promotional links  \n",
    "- URLs and email addresses  \n",
    "- Artifacts from deleted or removed posts  \n",
    "- HTML tags and escape sequences  \n",
    "- Spam posts identified during manual inspection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec020d8-a0b8-4d55-8edc-6a9b5add7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links\n",
    "df_processed[\"combined_text\"] = df_processed[\"combined_text\"].str.replace(r\"https?://\\S+\", \"\", regex=True)\n",
    "\n",
    "# Clean out unwanted patterns (deleted markers, HTML escapes/tags, email addresses)\n",
    "df_processed[\"combined_text\"] = df_processed[\"combined_text\"].str.replace(\"[deleted]\", \"\", regex=False)   # Remove [deleted] markers\n",
    "df_processed[\"combined_text\"] = df_processed[\"combined_text\"].str.replace(r\"&[^;]+;\", \"\", regex=True)     # Remove HTML escape sequences\n",
    "df_processed[\"combined_text\"] = df_processed[\"combined_text\"].str.replace(r\"</?\\w[^>]*>\", \"\", regex=True) # Remove HTML tags\n",
    "df_processed[\"combined_text\"] = df_processed[\"combined_text\"].str.replace(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", \"\", regex=True)  # Remove email addresses\n",
    "\n",
    "# Filter out spam identified by manual inspection\n",
    "df_processed = df_processed[~df_processed[\"selftext\"].str.contains(\"TEMU code glitch\")]\n",
    "\n",
    "# Check updated dataset\n",
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e626-11c7-4e4d-a354-a9e9134a9406",
   "metadata": {},
   "source": [
    "After cleaning, the dataset contains 5,244 posts with a `combined_text` column that will be the base for the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2c23e-7052-4d02-994a-395c5ca83baf",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. LDA Topic Modeling\n",
    "\n",
    "We create an LDA-specific copy of the dataset. For topic modeling, we remove most punctuation, lowercase everything but keep hyphens so hyphenated terms can remain intact for later tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf897124-f472-4987-a6ad-366c0d2fa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate copy for LDA-only preprocessing\n",
    "df_lda = df_processed.copy()\n",
    "\n",
    "# Remove punctuation/special chars but keep hyphens; also strip apostrophes so \"I'm\" -> \"I m\"\n",
    "df_lda[\"combined_text\"] = df_lda[\"combined_text\"].str.replace(r\"[^A-Za-z0-9\\s-]\", \" \", regex=True)  # Normalize punctuation\n",
    "\n",
    "# Lowercase text\n",
    "df_lda[\"combined_text\"] = df_lda[\"combined_text\"].str.lower()\n",
    "\n",
    "# Collapse multiple spaces introduced by replacements\n",
    "df_lda[\"combined_text\"] = df_lda[\"combined_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "df_lda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4d18a-6791-45df-990e-44462bcbdec8",
   "metadata": {},
   "source": [
    "### 4.1 Preparing DTM: Tokenization & Stopwords\n",
    "\n",
    "Now the text for the Document-Term Matrix (DTM) will be prepared in addition to assigning the stop word list. The steps are the following: \n",
    "- Sentence/word tokenization  \n",
    "- Filtering out non-alphabetic tokens  \n",
    "- Defining an extended stopword list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4c884-4556-4e63-af74-7d73bf9c44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources (will be skipped if already available)\n",
    "nltk.download(\"stopwords\", quiet=True) # English stopword list\n",
    "nltk.download(\"punkt\", quiet=True) # Sentence tokenizer data\n",
    "\n",
    "# Custom tokenizer: whitespace + keep only tokens containing letters\n",
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.ws = WhitespaceTokenizer()\n",
    "        self.letter_pat = r\"\\p{letter}\" # Matches letter\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        result = []\n",
    "        for sent in nltk.sent_tokenize(text): # Sentence split\n",
    "            tokens = self.ws.tokenize(sent) # Whitespace split\n",
    "            tokens = [t for t in tokens if regex.search(self.letter_pat, t)] # Keeps only tokens with letters\n",
    "            result.extend(tokens)\n",
    "        return result\n",
    "\n",
    "# Use the custom tokenizer\n",
    "mytokenizer = MyTokenizer()\n",
    "print(mytokenizer.tokenize(df_lda[\"combined_text\"].iloc[5]))  # Quick test of tokenizing a post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853c34b-41bf-48e9-9010-cc6bc036c5b1",
   "metadata": {},
   "source": [
    "We use NLTK’s English stopword list and extend it with a few additional words that are not included in the default list.  \n",
    "These additions cover leftover fragments from contractions (`\"im\"`, `\"ive\"`) as well as common filler words such as `\"like\"`, `\"get\"` and `\"got\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a987cd-f5fc-403c-ba20-af23c209d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the full NLTK stopword list\n",
    "print(stopwords.words(\"english\"))\n",
    "\n",
    "# Extend the list with additional custom stopwords\n",
    "stopwords_list = [\"like\", \"im\", \"ive\", \"get\", \"got\"] + stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b01e9f-7b98-4ae6-9346-913bdf36522f",
   "metadata": {},
   "source": [
    "### 4.2 Generating the Model\n",
    "\n",
    "We now move on to topic modeling using **Latent Dirichlet Allocation (LDA)**. Steps in this section include:  \n",
    "- Preparing the text data for LDA by creating a document-term matrix (DTM) with unigrams and bigrams\n",
    "- Converting the DTM into a format compatible with `gensim`\n",
    "- Fit multiple LDA models with different numbers of topics, passes and iterations to compare performance\n",
    "- Evaluating models using perplexity and coherence\n",
    "- Selecting the most promising models and inspect the top words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0405a0b-8dbd-455f-ac0e-9844eca55b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prepare text data (use cleaned text from df_lda)\n",
    "p_reddit = df_lda['combined_text'].astype(str)\n",
    "\n",
    "# Create DTM with unigrams and bigrams, filter by min_df and apply custom tokenizer and stopwords\n",
    "cv = CountVectorizer(ngram_range=(1, 2), min_df=0.01, stop_words=stopwords_list, tokenizer=mytokenizer.tokenize)\n",
    "dtm = cv.fit_transform(p_reddit)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230e0cd-face-44cd-aab5-f2b8ed755c24",
   "metadata": {},
   "source": [
    "The DTM contains 5,244 posts and 1,209 unique words. However, since LDA does not take a DTM as input, we first need to convert the DTM. The DTM also does not include feature names, so we store them separately in order to use them later in the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafea27b-747b-42f1-b42e-2de066b26517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM into gensim corpus format\n",
    "corpus = matutils.Sparse2Corpus(dtm, documents_columns=False)\n",
    "\n",
    "# Store vocab separately since DTM has no feature names\n",
    "vocab = dict(enumerate(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95080806-8a1c-40f1-bae5-a332ad7e9685",
   "metadata": {},
   "source": [
    "To determine a suitable number of topics and training parameters, we run several LDA models in a grid search. I vary the number of topics (`k`), the number of passes and the number of iterations, and record both perplexity and coherence scores for comparison. This step will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510a60b-6662-43fe-96bf-caaa93b5c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over different topic numbers, passes, and iterations\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in [5, 10, 15, 20]:  # Number of topics\n",
    "    for passes in [10, 20, 50]:  # Training passes\n",
    "        for iterations in [100, 200, 500]:  # Iterations\n",
    "            m = LdaModel(\n",
    "                corpus=corpus,\n",
    "                num_topics=k,\n",
    "                id2word=vocab,\n",
    "                random_state=123,\n",
    "                alpha=\"auto\",\n",
    "                passes=passes,\n",
    "                iterations=iterations,\n",
    "                eval_every=None\n",
    "            )\n",
    "            perplexity = m.log_perplexity(corpus)\n",
    "            coherence = CoherenceModel(\n",
    "                model=m, corpus=corpus, coherence=\"u_mass\"\n",
    "            ).get_coherence()\n",
    "            results.append(dict(\n",
    "                topics=k,\n",
    "                passes=passes,\n",
    "                iterations=iterations,\n",
    "                perplexity=perplexity,\n",
    "                coherence=coherence\n",
    "            ))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb7c7b-c57c-4359-b759-1d630bc56810",
   "metadata": {},
   "source": [
    "The model comparison shows that the best-performing setting is 5 topics with 10 passes and 100 iterations (based on our perplexity/coherence results). We now fit the LDA model with these parameters and inspect the top 10 words for each of the 5 topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a2eec-86f9-4479-b3ef-ae08455fd57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with 5 topics (baseline)\n",
    "lda = LdaModel(\n",
    "    corpus, id2word=vocab, num_topics=5, passes=10, iterations=100, random_state=123, alpha=\"auto\"\n",
    ")\n",
    "\n",
    "# Show the top 10 words for each of the 5 topics\n",
    "pd.DataFrame(\n",
    "    {f\"Topic {n}\": [w for (w, tw) in words] for (n, words) in lda.show_topics(formatted=False)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9e294-7188-4013-970c-7a451eb7331b",
   "metadata": {},
   "source": [
    "The resulting 5-topic solution looks acceptable but the topics are not very distinct and appear somewhat mixed. To improve interpretability, we also test a 10-topic model. According to the grid search, the highest coherence score was achieved with 10 topics, 20 passes and 200 iterations, although 100 iterations were enough to receive a result with meaningful topics and have a coherence score that is only lower by 0.004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e751e6b-a395-4297-8920-bb29be9a8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with 10 topics\n",
    "lda = LdaModel(\n",
    "    corpus, id2word=vocab, num_topics=10, passes=20, iterations=100, random_state=123, alpha=\"auto\"\n",
    ")\n",
    "\n",
    "# Show the top 10 words for each of the 10 topics\n",
    "pd.DataFrame(\n",
    "    {f\"Topic {n}\": [w for (w, tw) in words] for (n, words) in lda.show_topics(formatted=False)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726d877-2d59-4365-9dab-26e65467331b",
   "metadata": {},
   "source": [
    "The 10-topic solution looks more distinct and interpretable than the 5-topic model. Next, we extract the topic distributions for each document and combine them with the original metadata. This gives us a new dataset where every post is linked to its topic proportions, which will be useful for labeling topics and connecting them to the different frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e6615-eb91-409c-8d8e-4c73356f75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topic distributions across all documents\n",
    "topics = pd.DataFrame(\n",
    "    [dict(lda.get_document_topics(doc, minimum_probability=0.0)) for doc in corpus]\n",
    ")\n",
    "\n",
    "# Keep metadata for later analysis\n",
    "meta = df_lda.drop(columns=[\"combined_text\"]).copy()\n",
    "meta['text'] = p_reddit.reset_index(drop=True)  # Re-add cleaned text\n",
    "\n",
    "# Combine everything (topics + metadata)\n",
    "tpd = pd.concat([df_lda.reset_index(drop=True), topics], axis=1)\n",
    "tpd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bfe64-b147-4439-a5dc-03a5e031e73b",
   "metadata": {},
   "source": [
    "### 4.2 Analyze Topic Modeling Results\n",
    "\n",
    "To simplify interpretation, we assign each post to its most dominant topic. We also store the corresponding topic score and apply a threshold of `0.4`. If a post does not reach this minimum probability for any topic, it is not assigned to a specific topic. This helps reduce noise from posts that are too mixed across topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e648b9-2fc9-44e6-a857-fd2bde5d1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define topic columns (0–9 for the 10-topic model)\n",
    "topic_cols = list(range(10))\n",
    "\n",
    "# Assign each post to the topic with the highest probability\n",
    "tpd[\"top_topic\"] = tpd[topic_cols].idxmax(axis=1)\n",
    "\n",
    "# Store the corresponding maximum topic score\n",
    "tpd[\"top_topic_score\"] = tpd[topic_cols].max(axis=1)\n",
    "\n",
    "# Apply threshold: if the top topic score is below 0.4, set topic to None\n",
    "tpd.loc[tpd[\"top_topic_score\"] < 0.4, \"top_topic\"] = None\n",
    "\n",
    "# Show the distribution of assigned top topics (including None)\n",
    "print(tpd[\"top_topic\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2d351-8e62-44c6-ac2c-79dfe1348fe2",
   "metadata": {},
   "source": [
    "Now that each post has been assigned a top topic, we manually inspect a random sample of documents for a selected topic. This helps us qualitatively judge whether the model has grouped posts into coherent and meaningful themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00ca99-a2ac-43d0-858b-564080c07816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random sample of 20 posts, need to update \"top_topic\" depending on the topic interested in\n",
    "for r, row in tpd.loc[tpd[\"top_topic\"] == 0].sample(20, random_state=123)[[\"title\", \"selftext\"]].iterrows(): # Topic 0\n",
    "    print(row[\"title\"])      # Print the post title (unformated)\n",
    "    print(row[\"selftext\"])   # Print the post body (unformated)\n",
    "    print(\"---\")             # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90eed2b-4803-4ff9-b704-89b0ad582851",
   "metadata": {},
   "source": [
    "Based on the manual inspection of topics, we now map them into frames:  \n",
    "- Topic 1, 9: Individual framing \n",
    "- Topic 2, 5, 6: Structural\n",
    "- Topic 0, 4, 8: Political\n",
    "\n",
    "Posts that do not clearly belong to any of these categories (Topic 3, 7) or have no dominant topic are assigned to Unassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639dddd-9df4-4240-9f0e-fb51d36486f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics 1 and 9 to the \"Individual\" frame\n",
    "tpd.loc[tpd[\"top_topic\"].isin([1, 9]), \"frame\"] = \"Individual\"\n",
    "\n",
    "# Assign topics 2, 5, and 6 to the \"Structural\" frame\n",
    "tpd.loc[tpd[\"top_topic\"].isin([2, 5, 6]), \"frame\"] = \"Structural\"\n",
    "\n",
    "# Assign topics 0, 4, and 8 to the \"Political\" frame\n",
    "tpd.loc[tpd[\"top_topic\"].isin([0, 4, 8]), \"frame\"] = \"Political\"\n",
    "\n",
    "# Assign all other cases (including None/NaN) to \"Unassigned\"\n",
    "tpd.loc[~tpd[\"top_topic\"].isin([1, 9, 2, 4, 5, 6, 0, 8]) | tpd[\"top_topic\"].isna(), \"frame\"] = \"Unassigned\"\n",
    "\n",
    "# Show the distribution of assigned frames\n",
    "print(tpd[\"frame\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea49bb-d5c6-4b98-a8c9-62607300614a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sentiment Analysis\n",
    "\n",
    "I now analyze the emotional tone of the posts using VADER, a lexicon-based sentiment analysis tool. Each text is assigned a sentiment score between -1 (negative) and +1 (positive).  \n",
    "\n",
    "To better fit the context of this dataset, we adjust some words in the VADER lexicon that would otherwise bias the scores. For example, words like *like*, *free*, *please*, *rich* or *credit* often appear in descriptive contexts or as stop words, so I adjusted them to be neutral at a score of 0.0. This ensures the sentiment scores more accurately reflect the tone of the posts in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7c0f4-04aa-40eb-ada4-52105e058299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = df_processed.copy()\n",
    "\n",
    "# Load the VADER lexicon required for sentiment analysis\n",
    "nltk.download(\"vader_lexicon\", quiet = True)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Neutralize or adjust specific words to better fit the context\n",
    "custom_words = {\n",
    "    \"like\": 0.0,\n",
    "    \"free\": 0.0,\n",
    "    \"care\": 0.0,\n",
    "    \"please\": 0.0,\n",
    "    \"credit\": 0.0,\n",
    "    \"rich\": 0.0\n",
    "}\n",
    "analyzer.lexicon.update(custom_words)\n",
    "\n",
    "# Calculate sentiment scores for each text\n",
    "sentiments = []\n",
    "for text in df_sent[\"combined_text\"]:\n",
    "    score = analyzer.polarity_scores(str(text))[\"compound\"]\n",
    "    sentiments.append(score)\n",
    "\n",
    "# Add scores as new column\n",
    "df_sent[\"sentiment\"] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e1bd-822a-452c-9845-c4adce7870b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results\n",
    "\n",
    "Now I merge the sentiment scores with the topic and frame assignments, so that it's possible to compare average sentiment across topics and framing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931c2b3-d048-46f3-8064-fa58386441fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_sent.merge(\n",
    "    tpd[[\"id\", \"top_topic\", \"top_topic_score\", \"frame\"]],\n",
    "    on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77ccd5-9e19-4449-903d-2da2a35670ed",
   "metadata": {},
   "source": [
    "Compound scores of each topic and frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0f8a9-e447-4d68-82f5-3531552c5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.groupby(\"top_topic\")[\"sentiment\"].mean())\n",
    "print(\"--------------------\")\n",
    "print(merged_df.groupby(\"frame\")[\"sentiment\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264cc79-1622-4ba7-bdd6-b32e7c5a95f0",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Figures\n",
    "\n",
    "The next step is to visualize the results to better understand differences in sentiment across topics and frames.  \n",
    "Two bar plots are created:  \n",
    "\n",
    "1. **Average sentiment per topic**, with colors indicating the assigned frame. Horizontal lines mark neutral sentiment (0) and small thresholds (+0.05 and -0.05) to highlight deviations.  \n",
    "\n",
    "2. **Average sentiment per frame**, ordered by Political, Individual, and Structural. The same horizontal lines help interpret whether the average sentiment leans neutral, positive, or negative.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39f788-6377-4918-bede-d47be4e28437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose path for saving figures\n",
    "%cd \"/Users/dn/poverty-frames-reddit/03_manuscript/figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b35059-a70a-4eba-a64f-4c2ed5b4503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define consistent color palette for frames\n",
    "palette = {\n",
    "    \"Structural\": \"skyblue\",\n",
    "    \"Individual\": \"lightcoral\",\n",
    "    \"Political\": \"thistle\",\n",
    "    \"Unassigned\": \"silver\"\n",
    "}\n",
    "\n",
    "# Set the theme for a darker grid\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Figure a: Sentiment per Topic\n",
    "temp_df = merged_df[merged_df[\"top_topic\"].notna()].copy()\n",
    "temp_df[\"topic\"] = temp_df[\"top_topic\"] + 1  # Shift topic labels to 1–10opic\n",
    "\n",
    "sns.barplot(\n",
    "    data=temp_df,\n",
    "    x=\"topic\", y=\"sentiment\", hue=\"frame\", palette=palette,\n",
    "    err_kws={'linewidth': 1}, errorbar=('ci', False), edgecolor=\"none\"\n",
    ")\n",
    "\n",
    "plt.legend(title=\"Framing\", fontsize=9, facecolor=\"white\") # Legend, white background\n",
    "plt.axhline(0, color='black', linestyle='-', linewidth=1)      # Neutral baseline\n",
    "plt.axhline(0.05, color='green', linestyle='--', linewidth=1)  # Positive threshold\n",
    "plt.axhline(-0.05, color='red', linestyle='--', linewidth=1)   # Negative threshold\n",
    "plt.xlabel(\"Topic\", fontsize=13, labelpad=10)  # Rename label and add a bit of space\n",
    "plt.ylabel(\"Sentiment\", fontsize=13)  # Rename label\n",
    "\n",
    "\n",
    "plt.text(-0.15, 1, \"a\", transform=plt.gca().transAxes, fontsize=20, fontweight=\"bold\") # Letter (a)\n",
    "\n",
    "plt.savefig(\"average_sentiment_per_topic.pdf\", bbox_inches=\"tight\") # Save as PDF\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18360db-89ea-4f0e-a555-ecd12962d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure b: Sentiment per Frame\n",
    "sns.barplot(\n",
    "    data=merged_df,\n",
    "    x=\"frame\", y=\"sentiment\", hue=\"frame\", palette=palette,\n",
    "    err_kws={'linewidth': 1}, errorbar=('ci', False),\n",
    "    order=[\"Political\", \"Individual\", \"Structural\"], edgecolor=\"none\"\n",
    ")\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', linewidth=1)      # Neutral baseline\n",
    "plt.axhline(0.05, color='green', linestyle='--', linewidth=1)  # Positive threshold\n",
    "plt.axhline(-0.05, color='red', linestyle='--', linewidth=1)   # Negative threshold\n",
    "plt.xlabel(\"Framing\", fontsize = 13, labelpad = 10) # Rename label + add space\n",
    "plt.ylabel(\"Sentiment\", fontsize = 13) # Rename label\n",
    "\n",
    "plt.text(-0.15, 1, \"b\", transform=plt.gca().transAxes, fontsize=20, fontweight=\"bold\") # Letter (b)\n",
    "\n",
    "plt.savefig(\"average_sentiment_per_frame.pdf\", bbox_inches=\"tight\") # Save as PDF\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
